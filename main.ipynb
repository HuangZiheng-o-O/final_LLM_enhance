{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-03T08:04:21.323887Z",
     "start_time": "2024-09-03T08:04:21.280499Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import openai\n",
    "import yaml\n",
    "\n",
    "\n",
    "# Load API key from a YAML configuration file\n",
    "with open(\"config.yaml\") as f:\n",
    "    config_yaml = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "# Initialize the OpenAI client with the API key\n",
    "client = openai.OpenAI(api_key=config_yaml['token'])\n",
    "\n",
    "# Set the model name\n",
    "MODEL = \"gpt-4o-mini\"\n",
    "\n",
    "\n",
    "def llm(prompt, stop=[\"\\n\"]):\n",
    "    # Prepare the dialog for the API request\n",
    "    dialogs = [{\"role\": \"user\", \"content\": prompt}]\n",
    "\n",
    "    # Call OpenAI API to generate the completion\n",
    "    completion = client.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        messages=dialogs,\n",
    "        temperature=0,  # Controls randomness, 0 means more deterministic output\n",
    "        max_tokens=100,  # Limits the generated text length to 100 tokens\n",
    "        top_p=1,  # Controls diversity via nucleus sampling, 1 means using the full distribution\n",
    "        frequency_penalty=0.0,  # Reduces repeated words in the output, 0 means no penalty\n",
    "        presence_penalty=0.0,  # Reduces likelihood of word repetition, 0 means no penalty\n",
    "        stop=stop  # Stop sequence for generation, here it's a newline \"\\n\"\n",
    "    )\n",
    "\n",
    "    # Return the generated content\n",
    "    return completion.choices[0].message.content\n"
   ],
   "id": "aaf4666693c5e518",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-03T08:06:37.626278Z",
     "start_time": "2024-09-03T08:06:35.811191Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Assuming the LLM client and llm function are already defined as per the provided code.\n",
    "\n",
    "# Define the instruction to be analyzed\n",
    "instruction = \"足球运动员滑跪\"\n",
    "\n",
    "# Prompt for LLM to think and analyze the instruction\n",
    "think_prompt = f\"\"\"\n",
    "Determine whether the instruction '{instruction}' needs to be split into multiple consecutive actions. \n",
    "If splitting is required, identify the specific actions that should be created.\n",
    "\n",
    "You should think first before providing the response.\n",
    "When your thinking is done, say '<DONE>'\n",
    "Now think: \n",
    "\"\"\"\n",
    "\n",
    "# Function to interact with LLM for the 'think' step\n",
    "def analyze_instruction(instruction):\n",
    "    prompt = think_prompt + instruction\n",
    "    thought_action = llm(prompt, stop=[\"<DONE>\"])\n",
    "    \n",
    "    # Extract thought from the response\n",
    "    try:\n",
    "        thought = thought_action.strip().split(\"\\n\")[0]\n",
    "    except:\n",
    "        thought = thought_action.strip().split('\\n')[0]\n",
    "\n",
    "    # Display the thought process\n",
    "    print(f\"Thought: {thought}\")\n",
    "    \n",
    "    return thought\n",
    "\n",
    "# Function to determine if decomposition is needed\n",
    "def determine_decomposition(thought):\n",
    "    prompt = f\"Based on the thought '{thought}', does the instruction '{instruction}' need to be decomposed into multiple actions?\\nAnswer with 'True' or 'False':\"\n",
    "    response = llm(prompt, stop=[\"\\n\"])\n",
    "    \n",
    "    # Extract and return the true/false response\n",
    "    return response.strip()\n",
    "\n",
    "# Function to convert the response to a boolean\n",
    "def convert_to_bool(response):\n",
    "    if response.lower() == 'true':\n",
    "        return True\n",
    "    elif response.lower() == 'false':\n",
    "        return False\n",
    "    else:\n",
    "        raise ValueError(\"The response is not a valid boolean value.\")\n",
    "\n",
    "# Main execution\n",
    "thought = analyze_instruction(instruction)\n",
    "decomposition_needed = determine_decomposition(thought)\n",
    "decomposition_needed_bool = convert_to_bool(decomposition_needed)\n",
    "\n",
    "print(f\"Decomposition Needed: {decomposition_needed_bool}\")\n"
   ],
   "id": "93b97c7565e4de3e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thought: The instruction '足球运动员滑跪' translates to 'football player slides on knees'. This action can be interpreted as a single action, but it may involve multiple components: the football player preparing to slide, the actual sliding motion, and then coming to a stop on the knees.\n",
      "Decomposition Needed: True\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "4bce9dcdcfa3bee"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "66ed136821d91706"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
